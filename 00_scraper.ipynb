{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawler\n",
    "> This repository aims to explore the catalog available at wine.com.br, do some exploratory analysis in it and\n",
    "create initially a toy recommendation engine / wine classifier and pricing tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from fire import Fire\n",
    "from nbdev.showdoc import *\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from functools import partial\n",
    "from bs4 import BeautifulSoup as soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATALOG = \"//article\"\n",
    "NEXT = \"/html/body/div[6]/div/div[2]/div[2]/div/div[4]/div\"\n",
    "prefix = \"https://wine.com.br\"\n",
    "url_short = \"https://www.wine.com.br/browse.ep?cID=100851&exibirEsgotados=true&listagem=horizontal&sorter=featuredProducts-desc&filters=cVINHOS\"\n",
    "url_next = \"https://www.wine.com.br/browse.ep?cID=100851&exibirEsgotados=true&pn={page}&listagem=horizontal&sorter=featuredProducts-desc&filters=cVINHOS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatalogClassic(scrapy.Spider):\n",
    "    name = \"catalog_classic\"\n",
    "    #url = url_short\n",
    "    i=1\n",
    "\n",
    "    def start_requests(self):\n",
    "        yield scrapy.Request(url=url_short, callback=self.parse_page)\n",
    "\n",
    "    def parse_page(self, response, count=i):\n",
    "        wine_list = response.xpath(CATALOG)\n",
    "        for block in wine_list:\n",
    "            yield {\n",
    "                \"wine\": block.css('div > a::attr(\"title\")').get(),\n",
    "                \"link\": prefix + block.css('div > a::attr(\"href\")').get(),\n",
    "                \"país\": block.xpath('div[2]/div[1]/div/span').get()\n",
    "            }\n",
    "        #/html/body/div[5]/div/div[2]/div[2]/div/div[3]/ul/li[1]/article/div[2]/div[2]/div[1]/div/span\n",
    "        #next_page = response.xpath(f\"//a[./text()='Próxima >>']/@href\").get()\n",
    "        count +=1\n",
    "        next_page = url_next.format(page=count)\n",
    "        parse_next = partial(self.parse_page, count=count)\n",
    "        if count <= 431:\n",
    "            yield response.follow(next_page, parse_next)\n",
    "        \n",
    "\n",
    "    # Second parsing method\n",
    "    def parse_pages(self, response):\n",
    "        crs_title = response.xpath('//h1[contains(@class,\"title\")]/text()')\n",
    "        crs_title_ext = crs_title.extract_first().strip()\n",
    "        ch_titles = response.css(\"p.course__description::text\")\n",
    "        ch_titles_ext = [t.strip() for t in ch_titles.extract()]\n",
    "        dc_dict[crs_title_ext] = ch_titles_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class CatalogFaster(scrapy.Spider):\n",
    "    name = \"catalog\"\n",
    "    start_urls = [url_short] + [url_next.format(page=i) for i in range(2, 434)]\n",
    "    \n",
    "\n",
    "    def parse(self, response):\n",
    "        wine_list = response.xpath(CATALOG)\n",
    "        for block in wine_list:\n",
    "            yield {\n",
    "                \"wine\": block.css('div > a::attr(\"title\")').get(),\n",
    "                \"link\": prefix + block.css('a::attr(\"href\")').get(),\n",
    "                \"pontuação\": block.xpath('div[2]/div[2]/div[4]/div/div/div/div/span/text()').get(),\n",
    "                \"avaliações\": block.xpath('div[2]/div[2]/div[4]/div/a/text()').get(),\n",
    "                \"volume\": block.xpath('div[2]/div[2]/div[3]/text()').get()\n",
    "                \n",
    "            }\n",
    "    \n",
    "    # Second parsing method\n",
    "    def parse_pages(self, response):\n",
    "        crs_title = response.xpath('//h1[contains(@class,\"title\")]/text()')\n",
    "        crs_title_ext = crs_title.extract_first().strip()\n",
    "        ch_titles = response.css(\"p.course__description::text\")\n",
    "        ch_titles_ext = [t.strip() for t in ch_titles.extract()]\n",
    "        dc_dict[crs_title_ext] = ch_titles_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl():\n",
    "    # Run the Spider\n",
    "    process = CrawlerProcess()\n",
    "    process.crawl(CatalogFaster)\n",
    "    process.start()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    Fire(crawl)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "wino",
   "language": "python",
   "name": "wino"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
